import os
import sys
from fastapi import FastAPI, UploadFile, File, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from dotenv import load_dotenv
import uvicorn
from typing import Dict, Any, List
import tempfile
from pydantic import BaseModel
import base64
from openai import AzureOpenAI
import io
from PIL import Image

# Load environment variables from .env file
load_dotenv(override=True)

app = FastAPI(title="Document Intelligence API")

# Add CORS middleware to allow cross-origin requests
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Modify this in production to only allow specific origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Azure OpenAI client
client = AzureOpenAI(
    api_version=os.environ.get("AZURE_OPENAI_API_VERSION", "2024-12-01-preview"),
    azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT"),
    api_key=os.environ.get("AZURE_OPENAI_API_KEY")
)

class AnalysisResponse(BaseModel):
    words: List[Dict[str, Any]]
    lines: List[Dict[str, Any]]

class ImageAnalysisResponse(BaseModel):
    description: str

def get_azure_credentials():
    endpoint = os.environ.get("AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT")
    key = os.environ.get("AZURE_DOCUMENT_INTELLIGENCE_KEY")
    if not endpoint or not key:
        raise ValueError("Azure Document Intelligence credentials not found.")
    return endpoint, key

def analyze_document(endpoint, key, file_path, model_id="prebuilt-layout"):
    document_intelligence_client = DocumentIntelligenceClient(
        endpoint=endpoint,
        credential=AzureKeyCredential(key)
    )
    with open(file_path, "rb") as f:
        poller = document_intelligence_client.begin_analyze_document(
            model_id,
            body=f
        )
    result = poller.result()
    return result

def extract_text_and_coords(result):
    output = []
    for page in result.pages:
        page_number = page.page_number
        if hasattr(page, 'lines') and page.lines:
            for idx, line in enumerate(page.lines):
                text = line.content
                # Each line has a polygon (list of 8 floats: 4 points)
                polygon = line.polygon
                output.append({
                    'page': page_number,
                    'line_index' : idx,
                    'text': text,
                    'polygon': polygon
                })
    print(len(output))
    return output

def extract_words_and_coords(result):
    output = []
    for page in result.pages:
        page_number = page.page_number
        if hasattr(page, 'words') and page.words:
            for idx, word in enumerate(page.words):
                text = word.content
                polygon = word.polygon
                output.append({
                    'page': page_number,
                    'word_index': idx,
                    'text': text,
                    'polygon': polygon
                })
    print(len(output))
    return output

async def analyze_image_with_gpt4_vision(image_path, prompt="Describe this image in detail."):
    """
    Analyze an image using Azure OpenAI's GPT-4.1 vision capabilities
    
    Args:
        image_path: Path to the image file
        prompt: Instructions for the model about what to analyze in the image
        
    Returns:
        The text description generated by the model
    """
    # Get the deployment name from env vars
    deployment_name = os.environ.get("AZURE_OPENAI_DEPLOYMENT_NAME")
    
    # Read and encode the image as base64
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode('utf-8')
    
    try:
        # Call Azure OpenAI with the image using the new SDK format
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that can describe images accurately."},
                {
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
                    ]
                }
            ],
            max_tokens=2000
        )
        
        # Extract and return the response text
        return response.choices[0].message.content
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error from Azure OpenAI: {str(e)}")

@app.get("/")
async def root():
    return {"message": "Welcome to Document Intelligence API"}

@app.post("/analyze-pdf", response_model=AnalysisResponse)
async def analyze_pdf(file: UploadFile = File(...)):
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Only PDF files are supported")

    try:
        # Get Azure credentials
        endpoint, key = get_azure_credentials()

        # Save the uploaded file to a temporary location
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            temp_file.write(await file.read())
            temp_file_path = temp_file.name

        try:
            # Process the document
            result = analyze_document(endpoint, key, temp_file_path)

            # Extract words and lines with coordinates
            lines_coords = extract_text_and_coords(result)
            words_coords = extract_words_and_coords(result)

            # Create a proper response object that matches our model
            response = AnalysisResponse(
                words=words_coords,
                lines=lines_coords
            )

            return response

        finally:
            # Clean up the temporary file
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)

    except ValueError as e:
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

@app.post("/image-to-text", response_model=ImageAnalysisResponse)
async def image_to_text(file: UploadFile = File(...), prompt: str = "Describe this image in detail, focusing on any charts, graphs, or text visible."):
    """
    Endpoint to analyze an image using Azure OpenAI's GPT-4.1 vision capabilities
    
    Args:
        file: The uploaded image file
        prompt: Instructions for the model about what to analyze in the image
        
    Returns:
        A JSON response with the text description
    """
    # Check if the file is an image
    allowed_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
    file_ext = os.path.splitext(file.filename.lower())[1] if file.filename else ''
    
    if not any(file_ext.endswith(ext) for ext in allowed_extensions):
        raise HTTPException(status_code=400, detail="Only image files (jpg, jpeg, png, bmp) are supported")

    try:
        # Save the uploaded file to a temporary location
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_file:
            temp_file.write(await file.read())
            temp_file_path = temp_file.name

        try:
            # Process the image with GPT-4.1 Vision
            description = await analyze_image_with_gpt4_vision(temp_file_path, prompt)
            
            # Create a response
            response = ImageAnalysisResponse(description=description)
            return response
            
        finally:
            # Clean up the temporary file
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

@app.get("/analyze-stock-image")
async def analyze_stock_image(prompt: str = "Describe this stock chart in detail, including trends, patterns, and any relevant data points."):
    """
    Endpoint to analyze the included MSFT stock image using GPT-4.1 vision
    
    Args:
        prompt: Instructions for the model about what to analyze in the image
        
    Returns:
        A JSON response with the text description
    """
    try:
        # Path to the stock image
        stock_image_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "msft_stock.png")
        
        if not os.path.exists(stock_image_path):
            raise HTTPException(status_code=404, detail="Stock image not found")
            
        # Process the image with GPT-4.1 Vision
        description = await analyze_image_with_gpt4_vision(stock_image_path, prompt)
        
        # Create a response
        response = ImageAnalysisResponse(description=description)
        return response
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred: {str(e)}")

if __name__ == "__main__":
    uvicorn.run("api:app", host="0.0.0.0", port=8000, reload=True)